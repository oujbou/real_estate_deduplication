{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4ce93c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = False\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import re\n",
    "import joblib\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from config import config, hardware_config\n",
    "\n",
    "device = torch.device(\"mps\") if hardware_config.use_mps and torch.backends.mps.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b430d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Imputed listings: 1,428\n",
      "   Train pairs: 6,677\n",
      "   Test pairs: 1,563\n"
     ]
    }
   ],
   "source": [
    "## 1. Load Data and Prepare for Multi-Modal Processing\n",
    "\n",
    "# Load imputed dataset and training pairs\n",
    "listings_imputed = pd.read_csv(\"../data/processed/listings_imputed.csv\")\n",
    "train_pairs = pd.read_csv(\"../data/processed/train_pairs.csv\")\n",
    "test_pairs = pd.read_csv(\"../data/processed/test_pairs.csv\")\n",
    "\n",
    "print(f\"   Imputed listings: {len(listings_imputed):,}\")\n",
    "print(f\"   Train pairs: {len(train_pairs):,}\")\n",
    "print(f\"   Test pairs: {len(test_pairs):,}\")\n",
    "\n",
    "# Create lookup dictionary for fast listing access\n",
    "listings_dict = listings_imputed.set_index('listing_id').to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebc5b105",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For structured features used in the first model, no need for model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53afa365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_model = SentenceTransformer(config.text_model_name)\n",
    "# text_model.eval()\n",
    "\n",
    "# # Test encoding to verify model works\n",
    "# test_text = \"Appartement 3 pièces avec balcon à Conflans-Sainte-Honorine\"\n",
    "# test_encoding = text_model.encode(test_text)\n",
    "# print(f\"Text model loaded: {config.text_model_name}\")\n",
    "# print(f\"Embedding dimension: {len(test_encoding)}\")\n",
    "# print(f\"   Test encoding shape: {test_encoding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fdc278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialze model to process Text Data for Semantic Features\n",
    "\n",
    "if SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "    text_model = SentenceTransformer(config.text_model_name)\n",
    "    text_model.eval()\n",
    "    text_processor = text_model\n",
    "else:\n",
    "    text_model = TfidfVectorizer(\n",
    "        max_features=1000,\n",
    "        stop_words='french',\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.8\n",
    "    )\n",
    "    text_processor = text_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab8547b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image preprocessing verified: torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "## Initialze model to process image Data\n",
    "\n",
    "# Load pre-trained ResNet50\n",
    "image_model = getattr(models, config.image_model_name)(pretrained=True)\n",
    "image_model.eval()\n",
    "image_model = image_model.to(device)\n",
    "\n",
    "# Remove final classification layer to get features\n",
    "image_feature_extractor = nn.Sequential(*list(image_model.children())[:-1])\n",
    "\n",
    "# Image preprocessing pipeline\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Freature dimension is fixed for ResNet50: 2048\n",
    "# Test image processing\n",
    "def load_and_preprocess_image(image_path):\n",
    "    \"\"\"Load and preprocess single image\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        return image_transform(image).unsqueeze(0)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Test with sample images\n",
    "sample_images = list(Path(f\"../data/{config.pictures_dir}\").glob(\"*.jpg\"))[:1]\n",
    "if sample_images:\n",
    "    test_tensor = load_and_preprocess_image(sample_images[0])\n",
    "    if test_tensor is not None:\n",
    "        print(f\"Image preprocessing verified: {test_tensor.shape}\")\n",
    "else:\n",
    "    print(\"No sample images found for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e92a8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Structured features: 100%|██████████| 6677/6677 [00:00<00:00, 75521.59it/s]\n",
      "Structured features: 100%|██████████| 1563/1563 [00:00<00:00, 83276.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Train shape: (6677, 7)\n",
      "   Test shape: (1563, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Structured Feature Extraction\n",
    "\n",
    "structured_feature_names = [\n",
    "    'price_similarity', 'surface_similarity', 'price_per_m2_consistency',\n",
    "    'room_match', 'floor_similarity', 'floor_count_match', 'structured_consistency'\n",
    "]\n",
    "\n",
    "def extract_structured_features_pair(listing_A, listing_B):\n",
    "    \"\"\"\n",
    "    Extract 7 structured features comparing two listings\n",
    "    Returns: list of 7 numerical features\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Feature 1: Price similarity\n",
    "    price_A = listing_A.get('current_price', 0)\n",
    "    price_B = listing_B.get('current_price', 0)\n",
    "    \n",
    "    if price_A > 0 and price_B > 0:\n",
    "        price_sim = 1 - abs(price_A - price_B) / max(price_A, price_B)\n",
    "    else:\n",
    "        price_sim = 0.0\n",
    "    features.append(price_sim)\n",
    "    \n",
    "    # Feature 2: Surface similarity\n",
    "    surface_A = listing_A.get('surface_m2', 0)\n",
    "    surface_B = listing_B.get('surface_m2', 0)\n",
    "    \n",
    "    if surface_A > 0 and surface_B > 0:\n",
    "        surface_sim = 1 - abs(surface_A - surface_B) / max(surface_A, surface_B)\n",
    "    else:\n",
    "        surface_sim = 0.0\n",
    "    features.append(surface_sim)\n",
    "    \n",
    "    # Feature 3: Price per m² consistency\n",
    "    if all(x > 0 for x in [price_A, price_B, surface_A, surface_B]):\n",
    "        ppm2_A = price_A / surface_A\n",
    "        ppm2_B = price_B / surface_B\n",
    "        ppm2_sim = 1 - abs(ppm2_A - ppm2_B) / max(ppm2_A, ppm2_B)\n",
    "    else:\n",
    "        ppm2_sim = 0.0\n",
    "    features.append(ppm2_sim)\n",
    "    \n",
    "    # Feature 4: Room count match\n",
    "    room_A = listing_A.get('room_count', None)\n",
    "    room_B = listing_B.get('room_count', None)\n",
    "    \n",
    "    if room_A is not None and room_B is not None:\n",
    "        room_match = 1.0 if room_A == room_B else 0.0\n",
    "    else:\n",
    "        room_match = 0.5\n",
    "    features.append(room_match)\n",
    "    \n",
    "    # Feature 5: Floor similarity\n",
    "    floor_A = listing_A.get('floor', None)\n",
    "    floor_B = listing_B.get('floor', None)\n",
    "    \n",
    "    if floor_A is not None and floor_B is not None:\n",
    "        if floor_A == floor_B:\n",
    "            floor_sim = 1.0\n",
    "        else:\n",
    "            floor_diff = abs(float(floor_A) - float(floor_B))\n",
    "            floor_sim = max(0.0, 1.0 - floor_diff / 5.0)\n",
    "    else:\n",
    "        floor_sim = 0.5\n",
    "    features.append(floor_sim)\n",
    "    \n",
    "    # Feature 6: Floor count match\n",
    "    floor_count_A = listing_A.get('floor_count', None)\n",
    "    floor_count_B = listing_B.get('floor_count', None)\n",
    "    \n",
    "    if floor_count_A is not None and floor_count_B is not None:\n",
    "        floor_count_match = 1.0 if floor_count_A == floor_count_B else 0.0\n",
    "    else:\n",
    "        floor_count_match = 0.5\n",
    "    features.append(floor_count_match)\n",
    "    \n",
    "    # Feature 7: Overall structured consistency\n",
    "    valid_features = [f for f in features[:6] if f != 0.5 and f > 0]\n",
    "    consistency_score = np.mean(valid_features) if valid_features else 0.0\n",
    "    features.append(consistency_score)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_structured_features_batch(pairs_df, listings_dict):\n",
    "    \"\"\"Extract structured features for all pairs\"\"\"\n",
    "    feature_matrix = []\n",
    "    labels = []\n",
    "        \n",
    "    for _, row in tqdm(pairs_df.iterrows(), total=len(pairs_df), desc=\"Structured features\"):\n",
    "        try:\n",
    "            listing_A = listings_dict[row['listing_id_1']]\n",
    "            listing_B = listings_dict[row['listing_id_2']]\n",
    "            \n",
    "            features = extract_structured_features_pair(listing_A, listing_B)\n",
    "            feature_matrix.append(features)\n",
    "            labels.append(row['label'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed for pair {row['listing_id_1']}, {row['listing_id_2']}: {e}\")\n",
    "            feature_matrix.append([0.0] * 7)  # Zero vector for failed extractions\n",
    "            labels.append(row['label'])\n",
    "    \n",
    "    return np.array(feature_matrix), np.array(labels)\n",
    "\n",
    "# Extract structured features for train and test sets\n",
    "X_structured_train, y_train = extract_structured_features_batch(train_pairs, listings_dict)\n",
    "\n",
    "X_structured_test, y_test = extract_structured_features_batch(test_pairs, listings_dict)\n",
    "\n",
    "print(f\"   Train shape: {X_structured_train.shape}\")\n",
    "print(f\"   Test shape: {X_structured_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d236cf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Extracting text features for training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text features: 100%|██████████| 6677/6677 [02:58<00:00, 37.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Extracting text features for test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Text features: 100%|██████████| 1563/1563 [00:42<00:00, 36.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text features extraction complete:\n",
      "   Train features shape: (6677, 3)\n",
      "   Train embeddings shape: (6677, 384)\n",
      "   Test features shape: (1563, 3)\n",
      "   Method used: Sentence-BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Extract Text Features\n",
    "\n",
    "def extract_text_features_pair_bert(desc_A, desc_B, text_model):\n",
    "    \"\"\"Extract text features using Sentence-BERT\"\"\"\n",
    "\n",
    "    desc_A = str(desc_A) if pd.notna(desc_A) else \"\"\n",
    "    desc_B = str(desc_B) if pd.notna(desc_B) else \"\"\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # Get embeddings\n",
    "    if len(desc_A.strip()) > 0:\n",
    "        embedding_A = text_model.encode([desc_A])[0]\n",
    "    else:\n",
    "        embedding_A = np.zeros(text_model.get_sentence_embedding_dimension())\n",
    "    \n",
    "    if len(desc_B.strip()) > 0:\n",
    "        embedding_B = text_model.encode([desc_B])[0]\n",
    "    else:\n",
    "        embedding_B = np.zeros(text_model.get_sentence_embedding_dimension())\n",
    "    \n",
    "    features['embedding_A'] = embedding_A\n",
    "    features['embedding_B'] = embedding_B\n",
    "    \n",
    "    # Feature 1: Semantic similarity\n",
    "    if len(desc_A.strip()) > 0 and len(desc_B.strip()) > 0:\n",
    "        semantic_sim = cosine_similarity([embedding_A], [embedding_B])[0][0]\n",
    "    else:\n",
    "        semantic_sim = 0.0\n",
    "    features['semantic_similarity'] = float(semantic_sim)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_text_features_pair_tfidf(desc_A, desc_B, tfidf_vectorizer, fitted_vectors=None):\n",
    "    \"\"\"Extract text features using TF-IDF\"\"\"\n",
    "\n",
    "    desc_A = str(desc_A) if pd.notna(desc_A) else \"\"\n",
    "    desc_B = str(desc_B) if pd.notna(desc_B) else \"\"\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    if fitted_vectors is not None:\n",
    "        # Use pre-fitted vectorizer (for batch processing)\n",
    "        if len(desc_A.strip()) > 0 and len(desc_B.strip()) > 0:\n",
    "            vectors = tfidf_vectorizer.transform([desc_A, desc_B])\n",
    "            semantic_sim = cosine_similarity(vectors[0:1], vectors[1:2])[0][0]\n",
    "        else:\n",
    "            semantic_sim = 0.0\n",
    "    else:\n",
    "        # Fit on the fly (less efficient but works)\n",
    "        if len(desc_A.strip()) > 0 and len(desc_B.strip()) > 0:\n",
    "            vectors = tfidf_vectorizer.fit_transform([desc_A, desc_B])\n",
    "            semantic_sim = cosine_similarity(vectors[0:1], vectors[1:2])[0][0]\n",
    "        else:\n",
    "            semantic_sim = 0.0\n",
    "    \n",
    "    features['semantic_similarity'] = float(semantic_sim)\n",
    "    features['embedding_A'] = np.zeros(100)  # Placeholder for compatibility\n",
    "    features['embedding_B'] = np.zeros(100) \n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_common_text_features(desc_A, desc_B):\n",
    "    \"\"\"Extract features common to both approaches\"\"\"\n",
    "    desc_A = str(desc_A) if pd.notna(desc_A) else \"\"\n",
    "    desc_B = str(desc_B) if pd.notna(desc_B) else \"\"\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # Feature 2: Description length ratio\n",
    "    len_A = len(desc_A.strip())\n",
    "    len_B = len(desc_B.strip())\n",
    "    \n",
    "    if len_A > 0 and len_B > 0:\n",
    "        length_ratio = min(len_A, len_B) / max(len_A, len_B)\n",
    "    else:\n",
    "        length_ratio = 0.0\n",
    "    features['length_ratio'] = length_ratio\n",
    "    \n",
    "    # Feature 3: Real estate keyword overlap\n",
    "    domaine_keywords = {\n",
    "        'location': ['proche', 'gare', 'commerce', 'école', 'transport', 'métro', 'rer', 'bus', 'train', 'centre', 'quartier', 'commodité'],\n",
    "        'features': ['balcon', 'terrasse', 'cave', 'parking', 'ascenseur', 'gardien', 'jardin', 'cheminée', 'vis-a-vis'],\n",
    "        'condition': ['rénové', 'refait', 'neuf', 'ancien', 'lumineux', 'calme', 'isolé'],\n",
    "        'rooms': ['séjour', 'salon', 'cuisine', 'chambre', 'salle', 'bain', 'wc', 'dressing', 'hall', 'buanderie', 'grenier'],\n",
    "        'building': ['étage', 'immeuble', 'résidence', 'copropriété', 'charges', 'fermé', 'sécurisée']\n",
    "    }\n",
    "    \n",
    "    def extract_keywords(text):\n",
    "        text_lower = text.lower()\n",
    "        found_keywords = set()\n",
    "        for category, words in domaine_keywords.items():\n",
    "            for word in words:\n",
    "                if word in text_lower:\n",
    "                    found_keywords.add(word)\n",
    "        return found_keywords\n",
    "    \n",
    "    keywords_A = extract_keywords(desc_A)\n",
    "    keywords_B = extract_keywords(desc_B)\n",
    "    \n",
    "    if len(keywords_A) > 0 or len(keywords_B) > 0:\n",
    "        intersection = len(keywords_A.intersection(keywords_B))\n",
    "        union = len(keywords_A.union(keywords_B))\n",
    "        keyword_overlap = intersection / union if union > 0 else 0.0\n",
    "    else:\n",
    "        keyword_overlap = 0.0\n",
    "    features['keyword_overlap'] = keyword_overlap\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_text_features_batch(pairs_df, listings_dict, text_processor):\n",
    "    \"\"\"Extract text features for all pairs\"\"\"\n",
    "    all_features = []\n",
    "    embeddings_A = []\n",
    "    embeddings_B = []\n",
    "    labels = []\n",
    "        \n",
    "    # TF-IDF if transformer not available\n",
    "    if not SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "        all_descriptions = []\n",
    "        for _, row in pairs_df.iterrows():\n",
    "            try:\n",
    "                listing_A = listings_dict[row['listing_id_1']]\n",
    "                listing_B = listings_dict[row['listing_id_2']]\n",
    "                desc_A = str(listing_A.get('description', ''))\n",
    "                desc_B = str(listing_B.get('description', ''))\n",
    "                if len(desc_A.strip()) > 0:\n",
    "                    all_descriptions.append(desc_A)\n",
    "                if len(desc_B.strip()) > 0:\n",
    "                    all_descriptions.append(desc_B)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if all_descriptions:\n",
    "            text_processor.fit(all_descriptions)\n",
    "    \n",
    "    for _, row in tqdm(pairs_df.iterrows(), total=len(pairs_df), desc=\"Text features\"):\n",
    "        try:\n",
    "            listing_A = listings_dict[row['listing_id_1']]\n",
    "            listing_B = listings_dict[row['listing_id_2']]\n",
    "            \n",
    "            desc_A = listing_A.get('description', '')\n",
    "            desc_B = listing_B.get('description', '')\n",
    "            \n",
    "            # Extract features based on available method\n",
    "            if SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "                features = extract_text_features_pair_bert(desc_A, desc_B, text_processor)\n",
    "            else:\n",
    "                features = extract_text_features_pair_tfidf(desc_A, desc_B, text_processor)\n",
    "            \n",
    "            # Add common features\n",
    "            common_features = extract_common_text_features(desc_A, desc_B)\n",
    "            features.update(common_features)\n",
    "            \n",
    "            # Store computed features\n",
    "            text_feature_vector = [\n",
    "                features['semantic_similarity'],\n",
    "                features['length_ratio'],\n",
    "                features['keyword_overlap']\n",
    "            ]\n",
    "            all_features.append(text_feature_vector)\n",
    "            \n",
    "            # Store embeddings\n",
    "            embeddings_A.append(features['embedding_A'])\n",
    "            embeddings_B.append(features['embedding_B'])\n",
    "            labels.append(row['label'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed for pair {row['listing_id_1']}, {row['listing_id_2']}: {e}\")\n",
    "            all_features.append([0.0, 0.0, 0.0])\n",
    "            embeddings_A.append(np.zeros(100))\n",
    "            embeddings_B.append(np.zeros(100))\n",
    "            labels.append(row['label'])\n",
    "    \n",
    "    return {\n",
    "        'features': np.array(all_features),\n",
    "        'embeddings_A': np.array(embeddings_A),\n",
    "        'embeddings_B': np.array(embeddings_B),\n",
    "        'labels': np.array(labels)\n",
    "    }\n",
    "\n",
    "# Extract text features for train and test sets\n",
    "print(\"📊 Extracting text features for training set...\")\n",
    "text_train_data = extract_text_features_batch(train_pairs, listings_dict, text_processor)\n",
    "\n",
    "print(\"📊 Extracting text features for test set...\")\n",
    "text_test_data = extract_text_features_batch(test_pairs, listings_dict, text_processor)\n",
    "\n",
    "print(f\"Text features extraction complete:\")\n",
    "print(f\"   Train features shape: {text_train_data['features'].shape}\")\n",
    "print(f\"   Train embeddings shape: {text_train_data['embeddings_A'].shape}\")\n",
    "print(f\"   Test features shape: {text_test_data['features'].shape}\")\n",
    "print(f\"   Method used: {'Sentence-BERT' if SENTENCE_TRANSFORMERS_AVAILABLE else 'TF-IDF'}\")\n",
    "\n",
    "# Handle any NaN values\n",
    "text_train_data['features'] = np.nan_to_num(text_train_data['features'], nan=0.0)\n",
    "text_test_data['features'] = np.nan_to_num(text_test_data['features'], nan=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b707866b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image features: 100%|██████████| 6677/6677 [00:00<00:00, 22681.41it/s]\n",
      "Image features: 100%|██████████| 1563/1563 [00:00<00:00, 23801.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features extraction complete:\n",
      "  Train features shape: (6677, 2)\n",
      "  Train embeddings shape: (6677, 2048)\n",
      "  Test features shape: (1563, 2)\n",
      "  Features: 2 image similarity features + embeddings per pair\n"
     ]
    }
   ],
   "source": [
    "# Extract Image Features\n",
    "\n",
    "def get_listing_images(listing_id, pictures_dir, max_images=5):\n",
    "    picture_path = Path(pictures_dir)\n",
    "    image_paths = []\n",
    "    \n",
    "    # Try different naming patterns\n",
    "    for i in range(1, max_images + 1):\n",
    "        img_path = picture_path / f\"{listing_id}__{i}.jpg\"\n",
    "        if img_path.exists():\n",
    "            image_paths.append(img_path)\n",
    "    \n",
    "    # Try single image without suffix\n",
    "    if not image_paths:\n",
    "        img_path = picture_path / f\"{listing_id}_{i}.jpg\"\n",
    "        if img_path.exists():\n",
    "            image_paths.append(img_path)\n",
    "    \n",
    "    if not image_paths:\n",
    "        img_path = picture_path / f\"{listing_id}.jpg\"\n",
    "        if img_path.exists():\n",
    "            image_paths.append(img_path)\n",
    "    \n",
    "    return image_paths\n",
    "\n",
    "def extract_image_features_from_paths(image_paths, model, device, transform, batch_size=16):\n",
    "    \"\"\"Extract ResNet50 features from image paths\"\"\"\n",
    "    if not image_paths:\n",
    "        return np.array([])\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        batch_tensors = []\n",
    "        \n",
    "        for path in batch_paths:\n",
    "            tensor = load_and_preprocess_image(path)\n",
    "            if tensor is not None:\n",
    "                batch_tensors.append(tensor)\n",
    "        \n",
    "        if batch_tensors:\n",
    "            # Stack into batch and process\n",
    "            batch_tensor = torch.cat(batch_tensors, dim=0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                batch_features = model(batch_tensor)\n",
    "                batch_features = batch_features.view(batch_features.size(0), -1)  # Flatten\n",
    "                features.extend(batch_features.cpu().numpy())\n",
    "    \n",
    "    return np.array(features) if features else np.array([])\n",
    "\n",
    "def calculate_image_similarity_features(features_A, features_B):\n",
    "    \"\"\"Calculate max and average similarity between image feature sets\"\"\"\n",
    "    if len(features_A) == 0 or len(features_B) == 0:\n",
    "        return [0.0, 0.0]\n",
    "    \n",
    "    similarities = cosine_similarity(features_A, features_B)\n",
    "    \n",
    "    max_sim = np.max(similarities)\n",
    "    \n",
    "    avg_sim = np.mean(similarities)\n",
    "    \n",
    "    return [float(max_sim), float(avg_sim)]\n",
    "\n",
    "def extract_image_features_batch(pairs_df, listings_dict, image_model, device, transform, pictures_dir):\n",
    "    \"\"\"Extract image features for all pairs\"\"\"\n",
    "    all_features = []\n",
    "    all_embeddings_A = []\n",
    "    all_embeddings_B = []\n",
    "    labels = []\n",
    "        \n",
    "    for _, row in tqdm(pairs_df.iterrows(), total=len(pairs_df), desc=\"Image features\"):\n",
    "        try:\n",
    "            listing_id_A = row['listing_id_1']\n",
    "            listing_id_B = row['listing_id_2']\n",
    "            \n",
    "            # Get image paths\n",
    "            images_A = get_listing_images(listing_id_A, pictures_dir)\n",
    "            images_B = get_listing_images(listing_id_B, pictures_dir)\n",
    "            \n",
    "            # Extract features\n",
    "            features_A = extract_image_features_from_paths(\n",
    "                images_A, image_model, device, transform, hardware_config.image_batch_size\n",
    "            )\n",
    "            features_B = extract_image_features_from_paths(\n",
    "                images_B, image_model, device, transform, hardware_config.image_batch_size\n",
    "            )\n",
    "            \n",
    "            # Calculate similarity features\n",
    "            similarity_features = calculate_image_similarity_features(features_A, features_B)\n",
    "            all_features.append(similarity_features)\n",
    "            \n",
    "            # Store embeddings (use mean if multiple images)\n",
    "            if len(features_A) > 0:\n",
    "                embedding_A = np.mean(features_A, axis=0)\n",
    "            else:\n",
    "                embedding_A = np.zeros(2048)\n",
    "            \n",
    "            if len(features_B) > 0:\n",
    "                embedding_B = np.mean(features_B, axis=0)\n",
    "            else:\n",
    "                embedding_B = np.zeros(2048)\n",
    "            \n",
    "            all_embeddings_A.append(embedding_A)\n",
    "            all_embeddings_B.append(embedding_B)\n",
    "            labels.append(row['label'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Faied for pair {listing_id_A}, {listing_id_B}: {e}\")\n",
    "            all_features.append([0.0, 0.0])\n",
    "            all_embeddings_A.append(np.zeros(2048))\n",
    "            all_embeddings_B.append(np.zeros(2048))\n",
    "            labels.append(row['label'])\n",
    "    \n",
    "    return {\n",
    "        'features': np.array(all_features),\n",
    "        'embeddings_A': np.array(all_embeddings_A),\n",
    "        'embeddings_B': np.array(all_embeddings_B),\n",
    "        'labels': np.array(labels)\n",
    "    }\n",
    "\n",
    "# Extract image features for train and test sets\n",
    "pictures_dir = f\"../data/{config.pictures_dir}\"\n",
    "image_train_data = extract_image_features_batch(\n",
    "    train_pairs, listings_dict, image_feature_extractor, device, image_transform, pictures_dir\n",
    ")\n",
    "\n",
    "image_test_data = extract_image_features_batch(\n",
    "    test_pairs, listings_dict, image_feature_extractor, device, image_transform, pictures_dir\n",
    ")\n",
    "\n",
    "print(f\"Image features extraction complete:\")\n",
    "print(f\"  Train features shape: {image_train_data['features'].shape}\")\n",
    "print(f\"  Train embeddings shape: {image_train_data['embeddings_A'].shape}\")\n",
    "print(f\"  Test features shape: {image_test_data['features'].shape}\")\n",
    "print(f\"  Features: 2 image similarity features + embeddings per pair\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cba94cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Analysis and Visualization\n",
    "\n",
    "# Define feature names for all modalities (already defined for structured above)\n",
    "text_feature_names = [\n",
    "    'semantic_similarity', 'description_length_ratio', 'keyword_overlap'\n",
    "]\n",
    "\n",
    "image_feature_names = [\n",
    "    'max_image_similarity', 'avg_image_similarity'\n",
    "]\n",
    "\n",
    "\n",
    "# def analyze_modality_features(features, labels, feature_names, modality_name):\n",
    "#     \"\"\"Analyze feature distributions for a specific modality\"\"\"\n",
    "#     print(f\"\\n {modality_name.upper()} FEATURES ANALYSIS:\")\n",
    "#     print(\"-\" * 40)\n",
    "    \n",
    "#     pos_indices = labels == 1\n",
    "#     neg_indices = labels == 0\n",
    "    \n",
    "#     print(f\"Data shape: {features.shape}\")\n",
    "#     print(f\"Positive samples: {pos_indices.sum()}\")\n",
    "#     print(f\"Negative samples: {neg_indices.sum()}\")\n",
    "    \n",
    "#     # Feature statistics\n",
    "#     feature_stats = []\n",
    "#     for i, name in enumerate(feature_names):\n",
    "#         pos_mean = features[pos_indices, i].mean()\n",
    "#         neg_mean = features[neg_indices, i].mean()\n",
    "#         discrimination = abs(pos_mean - neg_mean)\n",
    "        \n",
    "#         feature_stats.append({\n",
    "#             'feature': name,\n",
    "#             'pos_mean': pos_mean,\n",
    "#             'neg_mean': neg_mean,\n",
    "#             'discrimination': discrimination,\n",
    "#             'overall_mean': features[:, i].mean(),\n",
    "#             'overall_std': features[:, i].std()\n",
    "#         })\n",
    "        \n",
    "#         print(f\"  {name:<25}: pos={pos_mean:.3f}, neg={neg_mean:.3f}, disc={discrimination:.3f}\")\n",
    "    \n",
    "#     return feature_stats\n",
    "\n",
    "# # Analyze each modality\n",
    "# structured_stats = analyze_modality_features(\n",
    "#     X_structured_train, y_train, structured_feature_names, \"Structured\"\n",
    "# )\n",
    "\n",
    "# text_stats = analyze_modality_features(\n",
    "#     text_train_data['features'], text_train_data['labels'], text_feature_names, \"Text\"\n",
    "# )\n",
    "\n",
    "# image_stats = analyze_modality_features(\n",
    "#     image_train_data['features'], image_train_data['labels'], image_feature_names, \"Image\"\n",
    "# )\n",
    "\n",
    "# # Create visualization of feature discrimination across modalities\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# # Plot discrimination scores for each modality\n",
    "# modalities = [\n",
    "#     (\"Structured\", structured_stats, structured_feature_names),\n",
    "#     (\"Text\", text_stats, text_feature_names),\n",
    "#     (\"Image\", image_stats, image_feature_names)\n",
    "# ]\n",
    "\n",
    "# for idx, (name, stats, feature_names_viz) in enumerate(modalities):\n",
    "#     ax = axes[idx]\n",
    "    \n",
    "#     discriminations = [s['discrimination'] for s in stats]\n",
    "#     colors = ['skyblue', 'lightcoral', 'lightgreen'][idx]\n",
    "    \n",
    "#     bars = ax.bar(range(len(discriminations)), discriminations, color=colors, alpha=0.7)\n",
    "#     ax.set_title(f'{name} Features Discrimination')\n",
    "#     ax.set_xlabel('Feature Index')\n",
    "#     ax.set_ylabel('Discrimination Score')\n",
    "#     ax.set_xticks(range(len(feature_names_viz)))\n",
    "#     ax.set_xticklabels([f'F{i+1}' for i in range(len(feature_names_viz))], rotation=45)\n",
    "#     ax.grid(True, alpha=0.3)\n",
    "    \n",
    "#     # Add value labels on bars\n",
    "#     for i, bar in enumerate(bars):\n",
    "#         height = bar.get_height()\n",
    "#         ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "#                 f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('../data/processed/modality_discrimination.png', dpi=150, bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# # Summary of best features by modality\n",
    "# print(f\"\\n🎯 TOP DISCRIMINATIVE FEATURES BY MODALITY:\")\n",
    "# for name, stats, _ in modalities:\n",
    "#     top_feature = max(stats, key=lambda x: x['discrimination'])\n",
    "#     print(f\"   {name:<12}: {top_feature['feature']} (discrimination: {top_feature['discrimination']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df4526c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "\n",
    "# Scale structured features for LightGBM\n",
    "structured_scaler = StandardScaler()\n",
    "X_structured_train_scaled = structured_scaler.fit_transform(X_structured_train)\n",
    "X_structured_test_scaled = structured_scaler.transform(X_structured_test)\n",
    "\n",
    "# Scale text features (for potential ML use beyond Sentence-BERT)\n",
    "text_scaler = StandardScaler()\n",
    "X_text_train_scaled = text_scaler.fit_transform(text_train_data['features'])\n",
    "X_text_test_scaled = text_scaler.transform(text_test_data['features'])\n",
    "\n",
    "# Scale image features \n",
    "image_scaler = StandardScaler()\n",
    "X_image_train_scaled = image_scaler.fit_transform(image_train_data['features'])\n",
    "X_image_test_scaled = image_scaler.transform(image_test_data['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c1f10ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured features saved: ../src/models/structured_features.npz\n",
      "Structured scaler saved: ../src/models/structured_scaler.pkl\n",
      "Saving text data components...\n",
      "Text features saved: ../src/models/text_features.npz\n",
      "Text scaler saved: ../src/models/text_scaler.pkl\n",
      "Text model: Load directly with SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
      "Saving image data components...\n",
      "Image features saved: ../src/models/image_features.npz\n",
      "Image scaler saved: ../src/models/image_scaler.pkl\n",
      "Image model saved: ../src/models/image_feature_extractor.pth\n",
      "Saving feature analysis...\n",
      "Feature analysis saved: ../src/models/feature_analysis.pkl\n",
      "Summary report saved: ../src/models/multimodal_summary.csv\n",
      "\n",
      "ALL ARTIFACTS SAVED TO: ../src/models\n"
     ]
    }
   ],
   "source": [
    "# Save feature set \n",
    "\n",
    "models_dir = Path(\"../src/models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save structured features and model componentsf\n",
    "np.savez_compressed(\n",
    "    models_dir / \"structured_features.npz\",\n",
    "    X_train=X_structured_train_scaled,\n",
    "    X_test=X_structured_test_scaled,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    feature_names=structured_feature_names\n",
    ")\n",
    "joblib.dump(structured_scaler, models_dir / \"structured_scaler.pkl\")\n",
    "print(f\"Structured features saved: {models_dir}/structured_features.npz\")\n",
    "print(f\"Structured scaler saved: {models_dir}/structured_scaler.pkl\")\n",
    "\n",
    "# Save text features and model components\n",
    "print(\"Saving text data components...\")\n",
    "np.savez_compressed(\n",
    "    models_dir / \"text_features.npz\",\n",
    "    X_train=X_text_train_scaled,\n",
    "    X_test=X_text_test_scaled,\n",
    "    embeddings_train_A=text_train_data['embeddings_A'],\n",
    "    embeddings_train_B=text_train_data['embeddings_B'],\n",
    "    embeddings_test_A=text_test_data['embeddings_A'],\n",
    "    embeddings_test_B=text_test_data['embeddings_B'],\n",
    "    y_train=text_train_data['labels'],\n",
    "    y_test=text_test_data['labels'],\n",
    "    feature_names=text_feature_names\n",
    ")\n",
    "joblib.dump(text_scaler, models_dir / \"text_scaler.pkl\")\n",
    "print(f\"Text features saved: {models_dir}/text_features.npz\")\n",
    "print(f\"Text scaler saved: {models_dir}/text_scaler.pkl\")\n",
    "print(f\"Text model: Load directly with SentenceTransformer('{config.text_model_name}')\")\n",
    "\n",
    "# Save image features and model components\n",
    "print(\"Saving image data components...\")\n",
    "np.savez_compressed(\n",
    "    models_dir / \"image_features.npz\",\n",
    "    X_train=X_image_train_scaled,\n",
    "    X_test=X_image_test_scaled,\n",
    "    embeddings_train_A=image_train_data['embeddings_A'],\n",
    "    embeddings_train_B=image_train_data['embeddings_B'],\n",
    "    embeddings_test_A=image_test_data['embeddings_A'],\n",
    "    embeddings_test_B=image_test_data['embeddings_B'],\n",
    "    y_train=image_train_data['labels'],\n",
    "    y_test=image_test_data['labels'],\n",
    "    feature_names=image_feature_names\n",
    ")\n",
    "joblib.dump(image_scaler, models_dir / \"image_scaler.pkl\")\n",
    "torch.save(image_feature_extractor.state_dict(), models_dir / \"image_feature_extractor.pth\")\n",
    "print(f\"Image features saved: {models_dir}/image_features.npz\")\n",
    "print(f\"Image scaler saved: {models_dir}/image_scaler.pkl\")\n",
    "print(f\"Image model saved: {models_dir}/image_feature_extractor.pth\")\n",
    "\n",
    "# Save comprehensive feature analysis\n",
    "print(\"Saving feature analysis...\")\n",
    "all_stats = {\n",
    "    'structured': structured_stats,\n",
    "    'text': text_stats, \n",
    "    'image': image_stats\n",
    "}\n",
    "joblib.dump(all_stats, models_dir / \"feature_analysis.pkl\")\n",
    "\n",
    "# Create summary report\n",
    "summary_data = {\n",
    "    'modality': ['Structured', 'Text', 'Image'],\n",
    "    'n_features': [len(structured_feature_names), len(text_feature_names), len(image_feature_names)],\n",
    "    'train_samples': [len(X_structured_train), len(X_text_train_scaled), len(X_image_train_scaled)],\n",
    "    'test_samples': [len(X_structured_test), len(X_text_test_scaled), len(X_image_test_scaled)],\n",
    "    'top_discrimination': [\n",
    "        max(structured_stats, key=lambda x: x['discrimination'])['discrimination'],\n",
    "        max(text_stats, key=lambda x: x['discrimination'])['discrimination'],\n",
    "        max(image_stats, key=lambda x: x['discrimination'])['discrimination']\n",
    "    ]\n",
    "}\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.to_csv(models_dir / \"multimodal_summary.csv\", index=False)\n",
    "\n",
    "print(f\"Feature analysis saved: {models_dir}/feature_analysis.pkl\")\n",
    "print(f\"Summary report saved: {models_dir}/multimodal_summary.csv\")\n",
    "\n",
    "print(f\"\\nALL ARTIFACTS SAVED TO: {models_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de24ab6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 IMAGE PROCESSING DIAGNOSTIC\n",
      "==============================\n",
      "Total .jpg files found: 12945\n",
      "Sample filenames: ['64024657__2.jpg', '107779728__6.jpg', '115841080__6.jpg', '122020856__12.jpg', '115841102__5.jpg']\n",
      "Listing 35031592: 0 images found\n",
      "  ❌ No images found for listing 35031592\n",
      "Listing 46868648: 0 images found\n",
      "  ❌ No images found for listing 46868648\n",
      "Listing 68728689: 0 images found\n",
      "  ❌ No images found for listing 68728689\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Check image processing\n",
    "print(\"🔍 IMAGE PROCESSING DIAGNOSTIC\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Check actual files in pictures directory\n",
    "pictures_dir = Path(f\"../data/{config.pictures_dir}\")\n",
    "all_image_files = list(pictures_dir.glob(\"*.jpg\"))\n",
    "print(f\"Total .jpg files found: {len(all_image_files)}\")\n",
    "\n",
    "if len(all_image_files) > 0:\n",
    "    print(f\"Sample filenames: {[f.name for f in all_image_files[:5]]}\")\n",
    "    \n",
    "    # Test with a few sample listings\n",
    "    sample_listing_ids = [35031592, 46868648, 68728689]  # From your train_pairs\n",
    "    \n",
    "    for listing_id in sample_listing_ids:\n",
    "        images_found = get_listing_images(listing_id, pictures_dir)\n",
    "        print(f\"Listing {listing_id}: {len(images_found)} images found\")\n",
    "        \n",
    "        if len(images_found) > 0:\n",
    "            # Test image loading\n",
    "            test_image = load_and_preprocess_image(images_found[0])\n",
    "            if test_image is not None:\n",
    "                print(f\"  ✅ Image {images_found[0].name} loaded successfully: {test_image.shape}\")\n",
    "                \n",
    "                # Test feature extraction\n",
    "                with torch.no_grad():\n",
    "                    features = image_feature_extractor(test_image.to(device))\n",
    "                    features = features.view(features.size(0), -1)\n",
    "                    print(f\"  ✅ Features extracted: {features.shape}, mean: {features.mean().item():.4f}\")\n",
    "            else:\n",
    "                print(f\"  ❌ Failed to load image {images_found[0].name}\")\n",
    "        else:\n",
    "            print(f\"  ❌ No images found for listing {listing_id}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No .jpg files found in pictures directory!\")\n",
    "    print(f\"Directory contents: {list(pictures_dir.iterdir())[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6cc5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "real_estate_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
