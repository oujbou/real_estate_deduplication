{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9022239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from config import config, hardware_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "026af3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df = pd.read_csv(f\"../data/{config.listings_file}\")\n",
    "duplicates_df = pd.read_csv(f\"../data/{config.duplicates_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf960ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 279 duplicate clusters\n",
      "Cluster size distribution: Counter({2: 98, 3: 53, 4: 33, 6: 27, 5: 16, 7: 12, 8: 11, 10: 6, 9: 6, 11: 4, 12: 3, 13: 3, 1: 2, 18: 2, 14: 1, 22: 1, 24: 1})\n",
      "\n",
      "Largest clusters: [24, 22, 18, 18, 14, 13, 13, 13, 12, 12]\n"
     ]
    }
   ],
   "source": [
    "# Rebuild connected components graph from EDA\n",
    "G = nx.Graph()\n",
    "for _, row in duplicates_df.iterrows():\n",
    "    G.add_edge(row['listing_id_1'], row['listing_id_2'])\n",
    "\n",
    "# Get connected components (clusters)\n",
    "clusters = list(nx.connected_components(G))\n",
    "cluster_sizes = [len(cluster) for cluster in clusters]\n",
    "\n",
    "print(f\"\\nFound {len(clusters)} duplicate clusters\")\n",
    "print(f\"Cluster size distribution: {Counter(cluster_sizes)}\")\n",
    "\n",
    "# Sort clusters by size for investigation\n",
    "clusters_sorted = sorted(clusters, key=len, reverse=True)\n",
    "print(f\"\\nLargest clusters: {[len(c) for c in clusters_sorted[:10]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "339cdf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_cluster(cluster, listings_df, cluster_id):\n",
    "    \"\"\"Detailed investigation of a specific cluster\"\"\"\n",
    "    print(f\"CLUSTER #{cluster_id} - Size: {len(cluster)} duplicates\")\n",
    "    \n",
    "    # Get cluster listings\n",
    "    cluster_listings = listings_df[listings_df['listing_id'].isin(cluster)].copy()\n",
    "    \n",
    "    if len(cluster_listings) == 0:\n",
    "        print(\"No listings found for this cluster (data quality issue)\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"\\n Found {len(cluster_listings)} actual listings in data\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nPRICE ANALYSIS:\")\n",
    "    prices = cluster_listings['current_price'].dropna()\n",
    "    if len(prices) > 0:\n",
    "        print(f\"   Price range: ‚Ç¨{prices.min():,.0f} - ‚Ç¨{prices.max():,.0f}\")\n",
    "        print(f\"   Price std: ‚Ç¨{prices.std():,.0f} ({100*prices.std()/prices.mean():.1f}% CV)\")\n",
    "    \n",
    "    print(f\"\\nSURFACE ANALYSIS:\")\n",
    "    surfaces = cluster_listings['surface_m2'].dropna()\n",
    "    if len(surfaces) > 0:\n",
    "        print(f\"   Surface range: {surfaces.min():.0f}m¬≤ - {surfaces.max():.0f}m¬≤\")\n",
    "        print(f\"   Surface std: {surfaces.std():.1f}m¬≤ ({100*surfaces.std()/surfaces.mean():.1f}% CV)\")\n",
    "    \n",
    "    print(f\"\\nPROPERTY CHARACTERISTICS:\")\n",
    "    room_counts = cluster_listings['room_count'].dropna().unique()\n",
    "    floors = cluster_listings['floor'].dropna().unique()\n",
    "    floor_counts = cluster_listings['floor_count'].dropna().unique()\n",
    "    \n",
    "    print(f\"   Room counts: {sorted(room_counts) if len(room_counts) > 0 else 'No data'}\")\n",
    "    print(f\"   Floors: {sorted(floors) if len(floors) > 0 else 'No data'}\")\n",
    "    print(f\"   Floor counts: {sorted(floor_counts) if len(floor_counts) > 0 else 'No data'}\")\n",
    "    \n",
    "    # Sample descriptions\n",
    "    descriptions = cluster_listings['description'].dropna()\n",
    "    print(f\"\\nDESCRIPTION SAMPLES:\")\n",
    "    for i, desc in enumerate(descriptions.head(3)):\n",
    "        print(f\"   Sample {i+1}: {desc[:150]}...\")\n",
    "    \n",
    "    # Quality assessment\n",
    "    price_cv = prices.std() / prices.mean() if len(prices) > 1 else 0\n",
    "    surface_cv = surfaces.std() / surfaces.mean() if len(surfaces) > 1 else 0\n",
    "    \n",
    "    is_suspicious = (\n",
    "        price_cv > 0.3 or  # >30% price variation\n",
    "        surface_cv > 0.3 or  # >30% surface variation\n",
    "        len(room_counts) > 2  # Too many different room counts\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nQUALITY ASSESSMENT:\")\n",
    "    print(f\"   Price CV: {price_cv:.3f} {'‚ö†Ô∏è' if price_cv > 0.3 else '‚úÖ'}\")\n",
    "    print(f\"   Surface CV: {surface_cv:.3f} {'‚ö†Ô∏è' if surface_cv > 0.3 else '‚úÖ'}\")\n",
    "    print(f\"   Room diversity: {'‚ö†Ô∏è' if len(room_counts) > 2 else '‚úÖ'}\")\n",
    "    print(f\"   Overall: {'SUSPICIOUS' if is_suspicious else '‚úÖ LEGITIMATE'}\")\n",
    "    \n",
    "    return not is_suspicious\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e91b5df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER #1 - Size: 24 duplicates\n",
      "\n",
      " Found 24 actual listings in data\n",
      "\n",
      "PRICE ANALYSIS:\n",
      "   Price range: ‚Ç¨259,900 - ‚Ç¨269,900\n",
      "   Price std: ‚Ç¨4,945 (1.9% CV)\n",
      "\n",
      "SURFACE ANALYSIS:\n",
      "   Surface range: 82m¬≤ - 82m¬≤\n",
      "   Surface std: 0.0m¬≤ (0.0% CV)\n",
      "\n",
      "PROPERTY CHARACTERISTICS:\n",
      "   Room counts: [4.0]\n",
      "   Floors: [3.0]\n",
      "   Floor counts: [3.0]\n",
      "\n",
      "DESCRIPTION SAMPLES:\n",
      "   Sample 1: * EXCLUSIVTE / CONFLANS* ! SECTEUR de FIN D'OISE, situ√© √† deux pas GARE RER (800 m√®tres) proche commodit√©s, sur secteur PRISE et RESIDENTIEL.!! dans R...\n",
      "   Sample 2: * EXCLUSIVTE / CONFLANS* ! SECTEUR de FIN D'OISE, situ√© √† deux pas GARE RER (800 m√®tres) proche commodit√©s, sur secteur PRISE et RESIDENTIEL.!! dans R...\n",
      "   Sample 3: * EXCLUSIVTE / CONFLANS* ! SECTEUR de FIN D'OISE, situ√© √† deux pas GARE RER (800 m√®tres) proche commodit√©s, sur secteur PRISE et RESIDENTIEL.!! dans R...\n",
      "\n",
      "QUALITY ASSESSMENT:\n",
      "   Price CV: 0.019 ‚úÖ\n",
      "   Surface CV: 0.000 ‚úÖ\n",
      "   Room diversity: ‚úÖ\n",
      "   Overall: ‚úÖ LEGITIMATE\n",
      "CLUSTER #2 - Size: 22 duplicates\n",
      "\n",
      " Found 22 actual listings in data\n",
      "\n",
      "PRICE ANALYSIS:\n",
      "   Price range: ‚Ç¨139,900 - ‚Ç¨145,000\n",
      "   Price std: ‚Ç¨1,791 (1.3% CV)\n",
      "\n",
      "SURFACE ANALYSIS:\n",
      "   Surface range: 52m¬≤ - 52m¬≤\n",
      "   Surface std: 0.0m¬≤ (0.0% CV)\n",
      "\n",
      "PROPERTY CHARACTERISTICS:\n",
      "   Room counts: [3.0]\n",
      "   Floors: [4.0]\n",
      "   Floor counts: [4.0]\n",
      "\n",
      "DESCRIPTION SAMPLES:\n",
      "   Sample 1: CONFLANS.! * EXCLUSIVITE * R√©s. PRIVATIVE √† deux pas des commodit√©s, GARES RER/SNCF, √©coles, commerces (ravalement r√©cent) BEAU T3 de 52m¬≤ Hab. BON ET...\n",
      "   Sample 2: CONFLANS.! * EXCLUSIVITE * R√©s. PRIVATIVE √† deux pas des commodit√©s, GARES RER/SNCF, √©coles, commerces (ravalement r√©cent) BEAU T3 de 52m¬≤ Hab. BON ET...\n",
      "   Sample 3: CONFLANS.! * EXCLUSIVITE * R√©s. PRIVATIVE √† deux pas des commodit√©s, GARES RER/SNCF, √©coles, commerces (ravalement r√©cent) BEAU T3 de 52m¬≤ Hab. BON ET...\n",
      "\n",
      "QUALITY ASSESSMENT:\n",
      "   Price CV: 0.013 ‚úÖ\n",
      "   Surface CV: 0.000 ‚úÖ\n",
      "   Room diversity: ‚úÖ\n",
      "   Overall: ‚úÖ LEGITIMATE\n",
      "CLUSTER #3 - Size: 18 duplicates\n",
      "\n",
      " Found 18 actual listings in data\n",
      "\n",
      "PRICE ANALYSIS:\n",
      "   Price range: ‚Ç¨170,000 - ‚Ç¨193,000\n",
      "   Price std: ‚Ç¨6,447 (3.4% CV)\n",
      "\n",
      "SURFACE ANALYSIS:\n",
      "   Surface range: 49m¬≤ - 51m¬≤\n",
      "   Surface std: 0.5m¬≤ (0.9% CV)\n",
      "\n",
      "PROPERTY CHARACTERISTICS:\n",
      "   Room counts: [2.0, 3.0]\n",
      "   Floors: [2.0]\n",
      "   Floor counts: [2.0]\n",
      "\n",
      "DESCRIPTION SAMPLES:\n",
      "   Sample 1: Vendu lou√©, Appartement f3 duplex dans residence de standing.Proche tous commerces avec 77.40 m¬≤ utiles comprenant entr√©e, salon, salle √† manger, cuis...\n",
      "   Sample 2: Exclusivit√© V2L Proche tous commerces tr√®s grand F2 en Duplex avec 77.40 m¬≤ utiles comprenant entr√©e, salon, salle √† manger, cuisine, salle de bains, ...\n",
      "   Sample 3: Exclusivit√© V2L Proche tous commerces tr√®s grand F2 en Duplex avec 77.40 m¬≤ utiles comprenant entr√©e, salon, salle √† manger, cuisine, salle de bains, ...\n",
      "\n",
      "QUALITY ASSESSMENT:\n",
      "   Price CV: 0.034 ‚úÖ\n",
      "   Surface CV: 0.009 ‚úÖ\n",
      "   Room diversity: ‚úÖ\n",
      "   Overall: ‚úÖ LEGITIMATE\n",
      "CLUSTER #4 - Size: 18 duplicates\n",
      "\n",
      " Found 18 actual listings in data\n",
      "\n",
      "PRICE ANALYSIS:\n",
      "   Price range: ‚Ç¨177,500 - ‚Ç¨189,000\n",
      "   Price std: ‚Ç¨4,247 (2.3% CV)\n",
      "\n",
      "SURFACE ANALYSIS:\n",
      "   Surface range: 65m¬≤ - 66m¬≤\n",
      "   Surface std: 0.2m¬≤ (0.4% CV)\n",
      "\n",
      "PROPERTY CHARACTERISTICS:\n",
      "   Room counts: [4.0]\n",
      "   Floors: [4.0]\n",
      "   Floor counts: [4.0]\n",
      "\n",
      "DESCRIPTION SAMPLES:\n",
      "   Sample 1: ** AGENCE WEELODGE ** Dans SECTEUR RECHERCHE de FIN D OISE proche RER A-SNCF, ECOLES et COMMERCES! Appartement de 4 pi√®ces TRES LUMINEUX situ√© au dern...\n",
      "   Sample 2: CONFLANS: A 2 PAS des GARES RER/ SNCF, ECOLES et commerces, dans r√©sidence r√©cemment raval√©e: SUPERBE F4 avec VUE IMPRENABLE comp: Entr√©e, double s√©jo...\n",
      "   Sample 3: ** AGENCE WEELODGE ** Dans SECTEUR RECHERCHE de FIN D OISE proche RER A-SNCF, ECOLES et COMMERCES! Appartement de 4 pi√®ces TRES LUMINEUX situ√© au dern...\n",
      "\n",
      "QUALITY ASSESSMENT:\n",
      "   Price CV: 0.023 ‚úÖ\n",
      "   Surface CV: 0.004 ‚úÖ\n",
      "   Room diversity: ‚úÖ\n",
      "   Overall: ‚úÖ LEGITIMATE\n",
      "CLUSTER #5 - Size: 14 duplicates\n",
      "\n",
      " Found 14 actual listings in data\n",
      "\n",
      "PRICE ANALYSIS:\n",
      "   Price range: ‚Ç¨154,000 - ‚Ç¨159,000\n",
      "   Price std: ‚Ç¨2,568 (1.6% CV)\n",
      "\n",
      "SURFACE ANALYSIS:\n",
      "   Surface range: 44m¬≤ - 44m¬≤\n",
      "   Surface std: 0.0m¬≤ (0.0% CV)\n",
      "\n",
      "PROPERTY CHARACTERISTICS:\n",
      "   Room counts: [3.0]\n",
      "   Floors: [3.0]\n",
      "   Floor counts: No data\n",
      "\n",
      "DESCRIPTION SAMPLES:\n",
      "   Sample 1: ** WEELODGE CONFLANS ** Dans une r√©sidence CALME, proches √©coles et transports, TRES BEL APPARTEMENT 3 pi√®ces. Il comprend une entr√©e, un s√©jour TRES ...\n",
      "   Sample 2: ** WEELODGE CONFLANS ** Dans une r√©sidence CALME, proches √©coles et transports, TRES BEL APPARTEMENT 3 pi√®ces. Il comprend une entr√©e, un s√©jour TRES ...\n",
      "   Sample 3: ** WEELODGE CONFLANS ** Dans une r√©sidence CALME, proches √©coles et transports, TRES BEL APPARTEMENT 3 pi√®ces. Il comprend une entr√©e, un s√©jour TRES ...\n",
      "\n",
      "QUALITY ASSESSMENT:\n",
      "   Price CV: 0.016 ‚úÖ\n",
      "   Surface CV: 0.000 ‚úÖ\n",
      "   Room diversity: ‚úÖ\n",
      "   Overall: ‚úÖ LEGITIMATE\n",
      "\n",
      "INVESTIGATION SUMMARY:\n",
      "   Legitimate clusters: 5\n",
      "   Suspicious clusters: 0\n"
     ]
    }
   ],
   "source": [
    "legitimate_clusters = []\n",
    "suspicious_clusters = []\n",
    "\n",
    "for i, cluster in enumerate(clusters_sorted[:5]):\n",
    "    is_legitimate = investigate_cluster(cluster, listings_df, i+1)\n",
    "    if is_legitimate:\n",
    "        legitimate_clusters.append(cluster)\n",
    "    else:\n",
    "        suspicious_clusters.append(cluster)\n",
    "\n",
    "print(f\"\\nINVESTIGATION SUMMARY:\")\n",
    "print(f\"   Legitimate clusters: {len(legitimate_clusters)}\")\n",
    "print(f\"   Suspicious clusters: {len(suspicious_clusters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdd5ab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_cluster_quality(cluster, listings_df):\n",
    "    \"\"\"Assess if a cluster is suitable for training data\"\"\"\n",
    "    cluster_listings = listings_df[listings_df['listing_id'].isin(cluster)]\n",
    "    \n",
    "    if len(cluster_listings) < 2:\n",
    "        return False, \"insufficient_data\"\n",
    "    \n",
    "    # Price consistency check\n",
    "    prices = cluster_listings['current_price'].dropna()\n",
    "    if len(prices) > 1:\n",
    "        price_cv = prices.std() / prices.mean()\n",
    "        if price_cv > 0.4:  # More than 40% price variation\n",
    "            return False, \"price_inconsistent\"\n",
    "    \n",
    "    # Surface consistency check\n",
    "    surfaces = cluster_listings['surface_m2'].dropna()\n",
    "    if len(surfaces) > 1:\n",
    "        surface_cv = surfaces.std() / surfaces.mean()\n",
    "        if surface_cv > 0.3:  # More than 30% surface variation\n",
    "            return False, \"surface_inconsistent\"\n",
    "    \n",
    "    # Room count consistency (allow 1-2 different room counts)\n",
    "    room_counts = cluster_listings['room_count'].nunique()\n",
    "    if room_counts > 2:\n",
    "        return False, \"room_inconsistent\"\n",
    "    \n",
    "    # Cluster size limit (avoid overly complex clusters)\n",
    "    if len(cluster) > 10:\n",
    "        return False, \"too_large\"\n",
    "    \n",
    "    return True, \"legitimate\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0809fdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Filtering all clusters for training data quality...\n",
      "\n",
      "FILTERING RESULTS:\n",
      "   Good legitimate: 260 clusters\n",
      "   Not good insufficient_data: 2 clusters\n",
      "   Not good too_large: 15 clusters\n",
      "   Not good price_inconsistent: 2 clusters\n",
      "\n",
      "TRAINING DATA SUMMARY:\n",
      "   Total original clusters: 279\n",
      "   Clean clusters for training: 260\n",
      "   Filter rate: 6.8%\n",
      "   Available positive pairs: 2,060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3v/l60wk4ln5w5c30qb9nzyj5140000gn/T/ipykernel_1074/1404290627.py:18: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  surface_cv = surfaces.std() / surfaces.mean()\n"
     ]
    }
   ],
   "source": [
    "clean_clusters = []\n",
    "filtered_stats = Counter()\n",
    "\n",
    "print(\"üîç Filtering all clusters for training data quality...\")\n",
    "\n",
    "for cluster in clusters:\n",
    "    is_clean, reason = assess_cluster_quality(cluster, listings_df)\n",
    "    filtered_stats[reason] += 1\n",
    "    \n",
    "    if is_clean:\n",
    "        clean_clusters.append(cluster)\n",
    "\n",
    "print(f\"\\nFILTERING RESULTS:\")\n",
    "for reason, count in filtered_stats.items():\n",
    "    result = \"Good\" if reason == \"legitimate\" else \"Not good\"\n",
    "    print(f\"   {result} {reason}: {count} clusters\")\n",
    "\n",
    "print(f\"\\nTRAINING DATA SUMMARY:\")\n",
    "print(f\"   Total original clusters: {len(clusters)}\")\n",
    "print(f\"   Clean clusters for training: {len(clean_clusters)}\")\n",
    "print(f\"   Filter rate: {100*(1-len(clean_clusters)/len(clusters)):.1f}%\")\n",
    "\n",
    "# Calculate clean training pairs\n",
    "total_clean_pairs = sum(len(list(combinations(cluster, 2))) for cluster in clean_clusters)\n",
    "print(f\"   Available positive pairs: {total_clean_pairs:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fa6b564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2,060 positive training pairs\n",
      "Average pairs per cluster: 7.9\n",
      "Positive pairs DataFrame shape: (2060, 4)\n",
      "   listing_id_1  listing_id_2  label  cluster_id\n",
      "0      64728971      65459581      1           0\n",
      "1      64728971      64721495      1           0\n",
      "2      65459581      64721495      1           0\n",
      "3      98429480      98430083      1           1\n",
      "4     117949690     117274350      1           2\n"
     ]
    }
   ],
   "source": [
    "def create_positive_pairs(clean_clusters):\n",
    "    \"\"\"Generate positive training pairs from clean clusters\"\"\"\n",
    "    positive_pairs = []\n",
    "    cluster_labels = []  # Track which cluster each pair comes from\n",
    "    \n",
    "    for cluster_id, cluster in enumerate(clean_clusters):\n",
    "        # Generate all possible pairs within cluster\n",
    "        cluster_pairs = list(combinations(cluster, 2))\n",
    "        positive_pairs.extend(cluster_pairs)\n",
    "        cluster_labels.extend([cluster_id] * len(cluster_pairs))\n",
    "    \n",
    "    return positive_pairs, cluster_labels\n",
    "\n",
    "positive_pairs, pair_cluster_labels = create_positive_pairs(clean_clusters)\n",
    "\n",
    "print(f\"Generated {len(positive_pairs):,} positive training pairs\")\n",
    "print(f\"Average pairs per cluster: {len(positive_pairs)/len(clean_clusters):.1f}\")\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "positive_df = pd.DataFrame({\n",
    "    'listing_id_1': [pair[0] for pair in positive_pairs],\n",
    "    'listing_id_2': [pair[1] for pair in positive_pairs],\n",
    "    'label': 1,\n",
    "    'cluster_id': pair_cluster_labels\n",
    "})\n",
    "\n",
    "print(f\"Positive pairs DataFrame shape: {positive_df.shape}\")\n",
    "print(positive_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd5ff6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating negative samples (ratio=3:1)\n",
      "Avoiding 2,060 existing positive pairs\n",
      "Total available listings for sampling: 1,428\n",
      "Generated 6,180 negative pairs in 6,214 attempts\n",
      "Success rate: 99.5%\n",
      "Negative pairs DataFrame shape: (6180, 4)\n"
     ]
    }
   ],
   "source": [
    "def create_negative_pairs(positive_pairs, listings_df, ratio=3):\n",
    "    \"\"\"\n",
    "    Generate negative pairs with random sampling strategy\n",
    "    - ratio: negative to positive ratio (3:1 recommended)\n",
    "    \"\"\"\n",
    "    print(f\"Creating negative samples (ratio={ratio}:1)\")\n",
    "    \n",
    "    # Create lookup for existing positive pairs\n",
    "    positive_set = set()\n",
    "    for pair in positive_pairs:\n",
    "        positive_set.add(tuple(sorted(pair)))\n",
    "    \n",
    "    print(f\"Avoiding {len(positive_set):,} existing positive pairs\")\n",
    "    \n",
    "    # Get all available listing IDs\n",
    "    all_listings = listings_df['listing_id'].tolist()\n",
    "    print(f\"Total available listings for sampling: {len(all_listings):,}\")\n",
    "    \n",
    "    negative_pairs = []\n",
    "    attempts = 0\n",
    "    max_attempts = len(positive_pairs) * ratio * 10  # Safety limit\n",
    "    \n",
    "    while len(negative_pairs) < len(positive_pairs) * ratio and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        \n",
    "        # random sampling from all listings\n",
    "        id1, id2 = random.sample(all_listings, 2)\n",
    "        \n",
    "        # Ensure not already a positive pair\n",
    "        pair_key = tuple(sorted([id1, id2]))\n",
    "        if pair_key not in positive_set:\n",
    "            negative_pairs.append((id1, id2))\n",
    "            positive_set.add(pair_key)  # Prevent duplicates in negatives too\n",
    "    \n",
    "    print(f\"Generated {len(negative_pairs):,} negative pairs in {attempts:,} attempts\")\n",
    "    print(f\"Success rate: {100*len(negative_pairs)/attempts:.1f}%\")\n",
    "    return negative_pairs\n",
    "\n",
    "# Generate negative pairs\n",
    "negative_pairs = create_negative_pairs(positive_pairs, listings_df, \n",
    "                                     ratio=config.negative_sampling_ratio)\n",
    "\n",
    "# Convert to DataFrame\n",
    "negative_df = pd.DataFrame({\n",
    "    'listing_id_1': [pair[0] for pair in negative_pairs],\n",
    "    'listing_id_2': [pair[1] for pair in negative_pairs],\n",
    "    'label': 0,\n",
    "    'cluster_id': -1  # No cluster for negatives\n",
    "})\n",
    "\n",
    "print(f\"Negative pairs DataFrame shape: {negative_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ae57513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL TRAINING DATASET:\n",
      "   Total pairs: 8,240\n",
      "   Positive pairs: 2,060 (25.0%)\n",
      "   Negative pairs: 6,180 (75.0%)\n"
     ]
    }
   ],
   "source": [
    "# Combine positive and negative pairs\n",
    "training_pairs_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "print(f\"\\nFINAL TRAINING DATASET:\")\n",
    "print(f\"   Total pairs: {len(training_pairs_df):,}\")\n",
    "print(f\"   Positive pairs: {len(positive_df):,} ({100*len(positive_df)/len(training_pairs_df):.1f}%)\")\n",
    "print(f\"   Negative pairs: {len(negative_df):,} ({100*len(negative_df)/len(training_pairs_df):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1406cdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing cluster-aware train/test split (test_size=0.2)\n",
      "Found 260 unique clusters to split\n",
      "   Train clusters: 208\n",
      "   Test clusters: 52\n",
      "\n",
      "SPLIT RESULTS:\n",
      "   Train set: 6,677 pairs\n",
      "   Test set: 1,563 pairs\n",
      "   Cluster overlap: 0 (NONE)\n",
      "\n",
      "LABEL DISTRIBUTION:\n",
      "   Train set:\n",
      "     Positive: 1,742\n",
      "     Negative: 4,935\n",
      "   Test set:\n",
      "     Positive: 318\n",
      "     Negative: 1,245\n"
     ]
    }
   ],
   "source": [
    "## 6. Cluster-Aware Train/Test Split\n",
    "\n",
    "def cluster_aware_split(training_pairs_df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split data ensuring no cluster appears in both train and test\n",
    "    This prevents data leakage in evaluation\n",
    "    \"\"\"\n",
    "    print(f\"Performing cluster-aware train/test split (test_size={test_size})\")\n",
    "    \n",
    "    # Get unique clusters (only for positive pairs)\n",
    "    positive_pairs = training_pairs_df[training_pairs_df['label'] == 1]\n",
    "    unique_clusters = positive_pairs['cluster_id'].unique()\n",
    "    \n",
    "    print(f\"Found {len(unique_clusters)} unique clusters to split\")\n",
    "    \n",
    "    # Split clusters into train/test\n",
    "    train_clusters, test_clusters = train_test_split(\n",
    "        unique_clusters, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"   Train clusters: {len(train_clusters)}\")\n",
    "    print(f\"   Test clusters: {len(test_clusters)}\")\n",
    "    \n",
    "    # Assign pairs to train/test based on cluster membership\n",
    "    def assign_split(row):\n",
    "        if row['label'] == 0:  # Negative pairs - random assignment\n",
    "            return 'test' if random.random() < test_size else 'train'\n",
    "        else:  # Positive pairs - based on cluster\n",
    "            return 'test' if row['cluster_id'] in test_clusters else 'train'\n",
    "    \n",
    "    # Set random seed for reproducible negative pair assignment\n",
    "    random.seed(random_state)\n",
    "    training_pairs_df['split'] = training_pairs_df.apply(assign_split, axis=1)\n",
    "    \n",
    "    # Create train/test DataFrames\n",
    "    train_df = training_pairs_df[training_pairs_df['split'] == 'train'].copy()\n",
    "    test_df = training_pairs_df[training_pairs_df['split'] == 'test'].copy()\n",
    "    \n",
    "    print(f\"\\nSPLIT RESULTS:\")\n",
    "    print(f\"   Train set: {len(train_df):,} pairs\")\n",
    "    print(f\"   Test set: {len(test_df):,} pairs\")\n",
    "    \n",
    "    # Verify no cluster overlap\n",
    "    train_pos_clusters = set(train_df[train_df['label']==1]['cluster_id'].unique())\n",
    "    test_pos_clusters = set(test_df[test_df['label']==1]['cluster_id'].unique())\n",
    "    overlap = train_pos_clusters.intersection(test_pos_clusters)\n",
    "    \n",
    "    print(f\"   Cluster overlap: {len(overlap)} ({'NONE' if len(overlap)==0 else 'FOUND'})\")\n",
    "    \n",
    "    # Label distribution\n",
    "    print(f\"\\nLABEL DISTRIBUTION:\")\n",
    "    print(\"   Train set:\")\n",
    "    print(f\"     Positive: {len(train_df[train_df['label']==1]):,}\")\n",
    "    print(f\"     Negative: {len(train_df[train_df['label']==0]):,}\")\n",
    "    print(\"   Test set:\")\n",
    "    print(f\"     Positive: {len(test_df[test_df['label']==1]):,}\")\n",
    "    print(f\"     Negative: {len(test_df[test_df['label']==0]):,}\")\n",
    "    \n",
    "    return train_df.drop('split', axis=1), test_df.drop('split', axis=1)\n",
    "\n",
    "# Perform cluster-aware split\n",
    "train_pairs, test_pairs = cluster_aware_split(\n",
    "    training_pairs_df, \n",
    "    test_size=config.train_test_split,\n",
    "    random_state=config.random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d868119f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ SAVED TRAINING DATA:\n",
      "   ../data/processed/train_pairs.csv (6,677 pairs)\n",
      "   ../data/processed/test_pairs.csv (1,563 pairs)\n",
      "   ../data/processed/clean_clusters.csv (260 clusters)\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(\"../data/processed\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save training pairs\n",
    "train_pairs.to_csv(output_dir / \"train_pairs.csv\", index=False)\n",
    "test_pairs.to_csv(output_dir / \"test_pairs.csv\", index=False)\n",
    "\n",
    "# Save cluster information for reference\n",
    "cluster_info = pd.DataFrame({\n",
    "    'cluster_id': range(len(clean_clusters)),\n",
    "    'cluster_size': [len(cluster) for cluster in clean_clusters],\n",
    "    'listing_ids': [list(cluster) for cluster in clean_clusters]\n",
    "})\n",
    "cluster_info.to_csv(output_dir / \"clean_clusters.csv\", index=False)\n",
    "\n",
    "print(\"üíæ SAVED TRAINING DATA:\")\n",
    "print(f\"   {output_dir}/train_pairs.csv ({len(train_pairs):,} pairs)\")\n",
    "print(f\"   {output_dir}/test_pairs.csv ({len(test_pairs):,} pairs)\")\n",
    "print(f\"   {output_dir}/clean_clusters.csv ({len(clean_clusters)} clusters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a60588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "real_estate_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
